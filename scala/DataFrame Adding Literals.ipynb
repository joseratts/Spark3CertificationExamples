{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook dedicado a explorar las distintas formas que existen para añadir valores constantes a columnas en un DataFrame\n",
    "* Función lit\n",
    "* Función typedLit\n",
    "* Utilizando SparkSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.1.36:4040\n",
       "SparkContext available as 'sc' (version = 3.0.2, master = local[*], app id = local-1615721746736)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---+------+------+\n",
      "|        name|age|weight|height|\n",
      "+------------+---+------+------+\n",
      "|        Jose| 40|    74|  1.63|\n",
      "|María Isabel| 38|    58|  1.65|\n",
      "|     Antonio| 27|    60|  1.68|\n",
      "|       Norma| 63|    65|  1.54|\n",
      "+------------+---+------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import spark.implicits._\n",
       "columns: Seq[String] = List(name, age, weight, height)\n",
       "data: Seq[(String, Int, Int, Double)] = List((Jose,40,74,1.63), (María Isabel,38,58,1.65), (Antonio,27,60,1.68), (Norma,63,65,1.54))\n",
       "df: org.apache.spark.sql.DataFrame = [name: string, age: int ... 2 more fields]\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._\n",
    "val columns = Seq(\"name\",\"age\",\"weight\", \"height\")\n",
    "val data = Seq((\"Jose\", 40, 74, 1.63), (\"María Isabel\", 38, 58, 1.65), (\"Antonio\", 27, 60, 1.68), (\"Norma\", 63, 65, 1.54))\n",
    "// Creando DF a partir de un Seq de datos utilizando implicits -> Seq.toDF\n",
    "// El schema es inferido y los nombres de columnas spasados como parametro\n",
    "val df = data.toDF(columns:_*)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---+------+------+------+\n",
      "|        name|age|weight|height|  type|\n",
      "+------------+---+------+------+------+\n",
      "|        Jose| 40|    74|  1.63|Person|\n",
      "|María Isabel| 38|    58|  1.65|Person|\n",
      "|     Antonio| 27|    60|  1.68|Person|\n",
      "|       Norma| 63|    65|  1.54|Person|\n",
      "+------------+---+------+------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.lit\n",
       "df2: org.apache.spark.sql.DataFrame = [name: string, age: int ... 3 more fields]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.lit\n",
    "                                       \n",
    "// Añadir una columna nueva y asigandole valores constante utilizando la función lit\n",
    "val df2 = df.withColumn(\"type\", lit(\"Person\"))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---+------+------+------+---------------------+\n",
      "|name        |age|weight|height|type  |citizenships         |\n",
      "+------------+---+------+------+------+---------------------+\n",
      "|Jose        |40 |74    |1.63  |Person|[Venezuelan, Spanish]|\n",
      "|María Isabel|38 |58    |1.65  |Person|[Venezuelan]         |\n",
      "|Antonio     |27 |60    |1.68  |Person|[Venezuelan]         |\n",
      "|Norma       |63 |65    |1.54  |Person|[Venezuelan]         |\n",
      "+------------+---+------+------+------+---------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{col, typedLit, when}\n",
       "df3: org.apache.spark.sql.DataFrame = [name: string, age: int ... 4 more fields]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{col, typedLit, when}\n",
    "                                       \n",
    "// Añadir una columna nueva y asigandole valores constante utilizando la función typedLit\n",
    "// La diferencia entre esta función y lit es que esta función puede manejar tipos de scala parametrizados, por ejemplo: List, Seq y Map\n",
    "val df3 = df2.withColumn(\"citizenships\", when(col(\"name\") === \"Jose\", typedLit(Seq(\"Venezuelan\", \"Spanish\")))\n",
    "                                        .otherwise(typedLit(Seq(\"Venezuelan\"))))\n",
    "df3.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---+------+------+------+---------------------+--------+\n",
      "|name        |age|weight|height|type  |citizenships         |location|\n",
      "+------------+---+------+------+------+---------------------+--------+\n",
      "|Jose        |40 |74    |1.63  |Person|[Venezuelan, Spanish]|SPAIN   |\n",
      "|María Isabel|38 |58    |1.65  |Person|[Venezuelan]         |SPAIN   |\n",
      "|Antonio     |27 |60    |1.68  |Person|[Venezuelan]         |SPAIN   |\n",
      "|Norma       |63 |65    |1.54  |Person|[Venezuelan]         |SPAIN   |\n",
      "+------------+---+------+------+------+---------------------+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df4: org.apache.spark.sql.DataFrame = [name: string, age: int ... 5 more fields]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Añadir una columna nueva asignandole valores constantes utilizando SparkSQL\n",
    "df3.createOrReplaceTempView(\"persons\")\n",
    "val df4 = spark.sql(\"SELECT name, age, weight, height, type, citizenships, 'SPAIN' AS location FROM persons\")\n",
    "df4.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
