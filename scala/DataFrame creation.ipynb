{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook dedicado a mostrar distintas formas de creación de DF a partir de datos en linea mediante:\n",
    "- RDD\n",
    "- Seq\n",
    "- Row\n",
    "- GenericRowWithSchema\n",
    "- GenericRow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.1.36:4044\n",
       "SparkContext available as 'sc' (version = 3.0.2, master = local[*], app id = local-1615723130945)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import spark.implicits._\n",
       "columns: Seq[String] = List(name, age, weight, height)\n",
       "data: Seq[(String, Int, Int, Double)] = List((Jose,40,74,1.63), (María Isabel,38,58,1.65), (Antonio,27,60,1.68), (Norma,63,65,1.54))\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// SparkSession inyectado al notebook por ende no es necesario instanciarlo\n",
    "\n",
    "import spark.implicits._\n",
    "val columns = Seq(\"name\",\"age\",\"weight\", \"height\")\n",
    "val data = Seq((\"Jose\", 40, 74, 1.63), (\"María Isabel\", 38, 58, 1.65), (\"Antonio\", 27, 60, 1.68), (\"Norma\", 63, 65, 1.54))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: integer (nullable = false)\n",
      " |-- _3: integer (nullable = false)\n",
      " |-- _4: double (nullable = false)\n",
      "\n",
      "+------------+---+---+----+\n",
      "|          _1| _2| _3|  _4|\n",
      "+------------+---+---+----+\n",
      "|        Jose| 40| 74|1.63|\n",
      "|María Isabel| 38| 58|1.65|\n",
      "|     Antonio| 27| 60|1.68|\n",
      "|       Norma| 63| 65|1.54|\n",
      "+------------+---+---+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[(String, Int, Int, Double)] = ParallelCollectionRDD[0] at parallelize at <console>:31\n",
       "dfFromRDD1: org.apache.spark.sql.DataFrame = [_1: string, _2: int ... 2 more fields]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Creando DF a partir de un RDD utilizando implicits -> rdd.toDF\n",
    "// El schema es inferido y los nombres de columnas son asignados por defecto: _1, _2, ...\n",
    "val rdd = spark.sparkContext.parallelize(data)\n",
    "val dfFromRDD1 = rdd.toDF()\n",
    "dfFromRDD1.printSchema()\n",
    "dfFromRDD1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = false)\n",
      " |-- weight: integer (nullable = false)\n",
      " |-- height: double (nullable = false)\n",
      "\n",
      "+------------+---+------+------+\n",
      "|        name|age|weight|height|\n",
      "+------------+---+------+------+\n",
      "|        Jose| 40|    74|  1.63|\n",
      "|María Isabel| 38|    58|  1.65|\n",
      "|     Antonio| 27|    60|  1.68|\n",
      "|       Norma| 63|    65|  1.54|\n",
      "+------------+---+------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dfFromRDD1WithColumnNames: org.apache.spark.sql.DataFrame = [name: string, age: int ... 2 more fields]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Creando DF a partir de un RDD utilizando implicits -> rdd.toDF\n",
    "// El schema es inferido y los nombres de columnas pasados como argumentos al método toDF\n",
    "val dfFromRDD1WithColumnNames = rdd.toDF(columns:_*)\n",
    "dfFromRDD1WithColumnNames.printSchema()\n",
    "dfFromRDD1WithColumnNames.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = false)\n",
      " |-- weight: integer (nullable = false)\n",
      " |-- height: double (nullable = false)\n",
      "\n",
      "+------------+---+------+------+\n",
      "|        name|age|weight|height|\n",
      "+------------+---+------+------+\n",
      "|        Jose| 40|    74|  1.63|\n",
      "|María Isabel| 38|    58|  1.65|\n",
      "|     Antonio| 27|    60|  1.68|\n",
      "|       Norma| 63|    65|  1.54|\n",
      "+------------+---+------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dfFromRDD2: org.apache.spark.sql.DataFrame = [name: string, age: int ... 2 more fields]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Creando DF a partir de método createDataFrame (de SparkSession) utilizando un RDD como argumento\n",
    "// El schema es inferido y los nombres de columnas pasados como argumentos al método toDF\n",
    "val dfFromRDD2 = spark.createDataFrame(rdd).toDF(columns:_*)\n",
    "dfFromRDD2.printSchema()\n",
    "dfFromRDD2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: integer (nullable = false)\n",
      " |-- _3: integer (nullable = false)\n",
      " |-- _4: double (nullable = false)\n",
      "\n",
      "+------------+---+---+----+\n",
      "|          _1| _2| _3|  _4|\n",
      "+------------+---+---+----+\n",
      "|        Jose| 40| 74|1.63|\n",
      "|María Isabel| 38| 58|1.65|\n",
      "|     Antonio| 27| 60|1.68|\n",
      "|       Norma| 63| 65|1.54|\n",
      "+------------+---+---+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dfFromData1: org.apache.spark.sql.DataFrame = [_1: string, _2: int ... 2 more fields]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Creando DF a partir de un Seq de datos utilizando implicits -> Seq.toDF\n",
    "// El schema es inferido y los nombres de columnas son asignados por defecto: _1, _2, ...\n",
    "val dfFromData1 = data.toDF()\n",
    "dfFromData1.printSchema()\n",
    "dfFromData1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = false)\n",
      " |-- weight: integer (nullable = false)\n",
      " |-- height: double (nullable = false)\n",
      "\n",
      "+------------+---+------+------+\n",
      "|        name|age|weight|height|\n",
      "+------------+---+------+------+\n",
      "|        Jose| 40|    74|  1.63|\n",
      "|María Isabel| 38|    58|  1.65|\n",
      "|     Antonio| 27|    60|  1.68|\n",
      "|       Norma| 63|    65|  1.54|\n",
      "+------------+---+------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dfFromData2: org.apache.spark.sql.DataFrame = [name: string, age: int ... 2 more fields]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Creando DF a partir de método createDataFrame (de SparkSession) utilizando un Seq como argumento\n",
    "// El schema es inferido y los nombres de columnas pasados como argumentos al método toDF\n",
    "var dfFromData2 = spark.createDataFrame(data).toDF(columns:_*)\n",
    "dfFromData2.printSchema()\n",
    "dfFromData2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- weight: integer (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      "\n",
      "+------------+---+------+------+\n",
      "|        name|age|weight|height|\n",
      "+------------+---+------+------+\n",
      "|        Jose| 40|    74|  1.63|\n",
      "|María Isabel| 38|    58|  1.65|\n",
      "|     Antonio| 27|    60|  1.68|\n",
      "|       Norma| 63|    65|  1.54|\n",
      "+------------+---+------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types.{StringType, IntegerType, DoubleType, StructField, StructType}\n",
       "import org.apache.spark.sql.Row\n",
       "schema: org.apache.spark.sql.types.StructType = StructType(StructField(name,StringType,true), StructField(age,IntegerType,true), StructField(weight,IntegerType,true), StructField(height,DoubleType,true))\n",
       "rowData: Seq[org.apache.spark.sql.Row] = List([Jose,40,74,1.63], [María Isabel,38,58,1.65], [Antonio,27,60,1.68], [Norma,63,65,1.54])\n",
       "dfFromData3: org.apache.spark.sql.DataFrame = [name: string, age: int ... 2 more fields]\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types.{StringType, IntegerType, DoubleType, StructField, StructType}\n",
    "import org.apache.spark.sql.Row\n",
    "\n",
    "val schema = StructType( Array(\n",
    "                 StructField(\"name\", StringType,true),\n",
    "                 StructField(\"age\", IntegerType,true),\n",
    "                 StructField(\"weight\", IntegerType,true),\n",
    "                 StructField(\"height\", DoubleType,true)\n",
    "             ))\n",
    "\n",
    "//From Data (USING createDataFrame and Adding schema using StructType)\n",
    "val rowData = Seq(Row(\"Jose\", 40, 74, 1.63), \n",
    "               Row(\"María Isabel\", 38, 58, 1.65), \n",
    "               Row(\"Antonio\", 27, 60, 1.68),\n",
    "               Row(\"Norma\", 63, 65, 1.54))\n",
    "\n",
    "// Otra forma, mas dinamica para generar rowData\n",
    "//val rowData = data.map(dataElement => Row.fromTuple(dataElement))\n",
    "\n",
    "// Creando DF a partir de método createDataFrame (de SparkSession) utilizando un RDD (sparkContext.parallelize) como argumento.\n",
    "// El RDD se creó a partir de un conjunto de instancias Row\n",
    "// El schema es pasado como parametro al crear el DataFrame\n",
    "val dfFromData3 = spark.createDataFrame(spark.sparkContext.parallelize(rowData),schema)\n",
    "dfFromData3.printSchema()\n",
    "dfFromData3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- weight: integer (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      "\n",
      "+------------+---+------+------+\n",
      "|        name|age|weight|height|\n",
      "+------------+---+------+------+\n",
      "|        Jose| 40|    74|  1.63|\n",
      "|María Isabel| 38|    58|  1.65|\n",
      "|     Antonio| 27|    60|  1.68|\n",
      "|       Norma| 63|    65|  1.54|\n",
      "+------------+---+------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema\n",
       "rowDataWithSchema: Seq[org.apache.spark.sql.Row] = List([Jose,40,74,1.63], [María Isabel,38,58,1.65], [Antonio,27,60,1.68], [Norma,63,65,1.54])\n",
       "dfFromData4: org.apache.spark.sql.DataFrame = [name: string, age: int ... 2 more fields]\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema\n",
    "\n",
    "val rowDataWithSchema: Seq[Row] = Seq(new GenericRowWithSchema(Array(\"Jose\", 40, 74, 1.63), schema), \n",
    "               new GenericRowWithSchema(Array(\"María Isabel\", 38, 58, 1.65), schema),\n",
    "               new GenericRowWithSchema(Array(\"Antonio\", 27, 60, 1.68), schema),\n",
    "               new GenericRowWithSchema(Array(\"Norma\", 63, 65, 1.54), schema))\n",
    "\n",
    "// Otra forma, mas dinamica para generar rowDataWithSchema\n",
    "// val rowDataWithSchema: Seq[Row] = data.map(dataElement => new GenericRowWithSchema(dataElement.productIterator.toArray, schema))\n",
    "\n",
    "\n",
    "\n",
    "// Creando DF a partir de método createDataFrame (de SparkSession) utilizando un RDD (sparkContext.parallelize) como argumento. \n",
    "// El RDD se creó a partir de un conjunto (Seq) de instancias GenericRowWithSchema\n",
    "// El schema es pasado como parametro al crear el DataFrame\n",
    "val dfFromData4 = spark.createDataFrame(spark.sparkContext.parallelize(rowDataWithSchema), schema)\n",
    "\n",
    "dfFromData4.printSchema()\n",
    "dfFromData4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- weight: integer (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      "\n",
      "+------------+---+------+------+\n",
      "|        name|age|weight|height|\n",
      "+------------+---+------+------+\n",
      "|        Jose| 40|    74|  1.63|\n",
      "|María Isabel| 38|    58|  1.65|\n",
      "|     Antonio| 27|    60|  1.68|\n",
      "|       Norma| 63|    65|  1.54|\n",
      "+------------+---+------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.catalyst.expressions.GenericRow\n",
       "rowDataWithoutSchema: Seq[org.apache.spark.sql.Row] = List([Jose,40,74,1.63], [María Isabel,38,58,1.65], [Antonio,27,60,1.68], [Norma,63,65,1.54])\n",
       "dfFromData5: org.apache.spark.sql.DataFrame = [name: string, age: int ... 2 more fields]\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.catalyst.expressions.GenericRow\n",
    "\n",
    "val rowDataWithoutSchema: Seq[Row] = data.map(dataElement => new GenericRow(dataElement.productIterator.toArray))\n",
    "\n",
    "\n",
    "\n",
    "// Creando DF a partir de método createDataFrame (de SparkSession) utilizando un RDD (sparkContext.parallelize) como argumento. \n",
    "// El RDD se creó a partir de un conjunto (Seq) de instancias GenericRow\n",
    "// El schema es pasado como parametro al crear el DataFrame\n",
    "val dfFromData5 = spark.createDataFrame(spark.sparkContext.parallelize(rowDataWithoutSchema), schema)\n",
    "\n",
    "dfFromData5.printSchema()\n",
    "dfFromData5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
