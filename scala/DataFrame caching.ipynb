{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook dedicado a explorar distintas formas de cache que se pueden aplicar sobre los datos de un DataFrame y algunos métodos de SparkSession.Catalog\n",
    "\n",
    "Spark gestiona los metadatos asociados a cada tabla (gestionada o no). Esto se captura en el Catálogo (SparkSession.Catalog), Catalog es una abstracción de alto nivel en Spark SQL para almacenar metadatos. La funcionalidad del catálogo se amplió en Spark 2.x con nuevos métodos públicos que le permiten examinar los metadatos asociados con sus bases de datos, tablas y vistas. Spark 3.0 lo amplía para usar un catálogo externo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.1.36:4042\n",
       "SparkContext available as 'sc' (version = 3.0.2, master = local[*], app id = local-1615722556779)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import spark.implicits._\n",
       "columns: Seq[String] = List(name, age, weight, height)\n",
       "data: Seq[(String, Int, Int, Double)] = List((Jose,40,74,1.63), (María Isabel,38,58,1.65), (Antonio,27,60,1.68), (Norma,63,65,1.54))\n",
       "df: org.apache.spark.sql.DataFrame = [name: string, age: int ... 2 more fields]\n",
       "catalog: org.apache.spark.sql.catalog.Catalog = org.apache.spark.sql.internal.CatalogImpl@47163b1a\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._\n",
    "val columns = Seq(\"name\",\"age\",\"weight\", \"height\")\n",
    "val data = Seq((\"Jose\", 40, 74, 1.63), (\"María Isabel\", 38, 58, 1.65), (\"Antonio\", 27, 60, 1.68), (\"Norma\", 63, 65, 1.54))\n",
    "val df = data.toDF(columns:_*)\n",
    "\n",
    "// declaramos variable para referenciar el catalogo\n",
    "val catalog = spark.catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project [unresolvedalias('name, None), unresolvedalias('age, None), unresolvedalias('weight, None)]\n",
      "+- Project [_1#4 AS name#13, _2#5 AS age#14, _3#6 AS weight#15, _4#7 AS height#16]\n",
      "   +- LocalRelation [_1#4, _2#5, _3#6, _4#7]\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "name: string, age: int, weight: int\n",
      "Project [name#13, age#14, weight#15]\n",
      "+- Project [_1#4 AS name#13, _2#5 AS age#14, _3#6 AS weight#15, _4#7 AS height#16]\n",
      "   +- LocalRelation [_1#4, _2#5, _3#6, _4#7]\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [name#13, age#14, weight#15]\n",
      "+- InMemoryRelation [name#13, age#14, weight#15, height#16], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "      +- LocalTableScan [name#13, age#14, weight#15, height#16]\n",
      "\n",
      "== Physical Plan ==\n",
      "InMemoryTableScan [name#13, age#14, weight#15]\n",
      "   +- InMemoryRelation [name#13, age#14, weight#15, height#16], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "         +- LocalTableScan [name#13, age#14, weight#15, height#16]\n",
      "\n",
      "+------------+---+------+\n",
      "|        name|age|weight|\n",
      "+------------+---+------+\n",
      "|        Jose| 40|    74|\n",
      "|María Isabel| 38|    58|\n",
      "|     Antonio| 27|    60|\n",
      "|       Norma| 63|    65|\n",
      "+------------+---+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df1: org.apache.spark.sql.DataFrame = [name: string, age: int ... 1 more field]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cache() // cache la data\n",
    "df.count() // Acción que materializa la cache\n",
    "\n",
    "\n",
    "val df1 = df.select(\"name\",\"age\",\"weight\")\n",
    "// Se puede apreciar que los datos están cacheados al revisar el plan fisico y aparecer InMemoryTableScan\n",
    "df1.explain(true)\n",
    "df1.show() \n",
    "\n",
    "//df.unpersist() // Se elimina la cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-----------+---------+-----------+\n",
      "|name|database|description|tableType|isTemporary|\n",
      "+----+--------+-----------+---------+-----------+\n",
      "+----+--------+-----------+---------+-----------+\n",
      "\n",
      "+-------+--------+-----------+---------+-----------+\n",
      "|   name|database|description|tableType|isTemporary|\n",
      "+-------+--------+-----------+---------+-----------+\n",
      "|persons|    null|       null|TEMPORARY|       true|\n",
      "+-------+--------+-----------+---------+-----------+\n",
      "\n",
      "+------------+---+------+------+------+\n",
      "|        name|age|weight|height|  type|\n",
      "+------------+---+------+------+------+\n",
      "|        Jose| 40|    74|  1.63|person|\n",
      "|María Isabel| 38|    58|  1.65|person|\n",
      "|     Antonio| 27|    60|  1.68|person|\n",
      "|       Norma| 63|    65|  1.54|person|\n",
      "+------------+---+------+------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.lit\n",
       "df2: org.apache.spark.sql.DataFrame = [name: string, age: int ... 3 more fields]\n",
       "res1: Boolean = true\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.lit\n",
    "\n",
    "val df2 = df.withColumn(\"type\", lit(\"person\"))\n",
    "\n",
    "// Muestra listado de tablas\n",
    "catalog.listTables.show()\n",
    "\n",
    "// Comprobamos si existe la tabla o vista con el nombre especificado\n",
    "if (catalog.tableExists(\"persons\")){\n",
    "  // Eliminamos la vista\n",
    "  catalog.dropTempView(\"persons\")\n",
    "}\n",
    "\n",
    "// Creamos la vista\n",
    "// NOTA: Crear la vista no significa que se haga cache sobre los datos\n",
    "df2.createTempView(\"persons\")\n",
    "\n",
    "catalog.listTables.show()\n",
    "\n",
    "// Cache de la vista mediante Spark SQL. Realiza el cache inmediatamente\n",
    "spark.sql(\"cache table persons\")\n",
    "spark.sql(\"SELECT * FROM persons\").show()\n",
    "// Se elimina la Cache de la vista mediante Spark SQL\n",
    "spark.sql(\"uncache table persons\")\n",
    "\n",
    "catalog.dropTempView(\"persons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---+------+------+------+------+\n",
      "|        name|age|weight|height|  type|   sex|\n",
      "+------------+---+------+------+------+------+\n",
      "|        Jose| 40|    74|  1.63|person|  Male|\n",
      "|María Isabel| 38|    58|  1.65|person|Female|\n",
      "|     Antonio| 27|    60|  1.68|person|  Male|\n",
      "|       Norma| 63|    65|  1.54|person|Female|\n",
      "+------------+---+------+------+------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{when, col}\n",
       "df3: org.apache.spark.sql.DataFrame = [name: string, age: int ... 4 more fields]\n",
       "res2: Boolean = true\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{when, col}\n",
    "\n",
    "val df3 = df2.withColumn(\"sex\", when(col(\"name\") === \"Jose\" || col(\"name\") === \"Antonio\", \"Male\").otherwise(\"Female\"))\n",
    "\n",
    "if (catalog.tableExists(\"persons2\")){\n",
    "  catalog.dropTempView(\"persons2\")\n",
    "}\n",
    "\n",
    "df3.createTempView(\"persons2\")\n",
    "\n",
    "// Cache lazy de la vista mediante Spark SQL\n",
    "// En Spark 3.0, además de otras opciones, puede especificar una tabla como LAZY, lo que significa que solo debe almacenarse en caché cuando se usa por primera vez\n",
    "// en lugar de inmediatamente\n",
    "spark.sql(\"cache lazy table persons2\")\n",
    "spark.sql(\"SELECT * FROM persons2\").show()\n",
    "\n",
    "// Se elimina la Cache de la vista mediante Spark SQL\n",
    "spark.sql(\"uncache table persons2\")\n",
    "\n",
    "catalog.dropTempView(\"persons2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is person3 cached -> false\n",
      "Is person3 cached -> true\n",
      "+------------+---+------+------+------+------+---------+\n",
      "|        name|age|weight|height|  type|   sex|  country|\n",
      "+------------+---+------+------+------+------+---------+\n",
      "|        Jose| 40|    74|  1.63|person|  Male|Venezuela|\n",
      "|María Isabel| 38|    58|  1.65|person|Female|Venezuela|\n",
      "|     Antonio| 27|    60|  1.68|person|  Male|Venezuela|\n",
      "|       Norma| 63|    65|  1.54|person|Female|Venezuela|\n",
      "+------------+---+------+------+------+------+---------+\n",
      "\n",
      "Is person3 cached -> false\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df4: org.apache.spark.sql.DataFrame = [name: string, age: int ... 5 more fields]\n",
       "res3: Boolean = true\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df4 = df3.withColumn(\"country\", lit(\"Venezuela\"))\n",
    "if (catalog.tableExists(\"persons3\")){\n",
    "  catalog.dropTempView(\"persons3\")\n",
    "}\n",
    "\n",
    "df4.createTempView(\"persons3\")\n",
    "\n",
    "// Comprobamos que la vista este cacheada\n",
    "println(s\"Is person3 cached -> ${catalog.isCached(\"persons3\")}\")\n",
    "\n",
    "// Almacena en memoria caché la tabla/vista especificada.\n",
    "catalog.cacheTable(\"persons3\")\n",
    "\n",
    "// Comprobamos que la vista está en caché\n",
    "println(s\"Is person3 cached -> ${catalog.isCached(\"persons3\")}\")\n",
    "\n",
    "// Intentamos almacenar en caché los datos de una vista ya previamente guardados (en caché)\n",
    "spark.sql(\"cache lazy table persons3\")\n",
    "spark.sql(\"SELECT * FROM persons3\").show()\n",
    "\n",
    "// Se elimina la Cache de la vista mediante Spark SQL, que fue cacheada mediante la función de SparkSession.Catalog.cacheTable\n",
    "spark.sql(\"uncache table persons3\")\n",
    "// Si intentamos eliminar la cache de una tabla/vista que no existe nos arroja => AnalysisException: Table or view not found: default.persons4;\n",
    "//spark.sql(\"uncache table persons4\")\n",
    "\n",
    "// Comprobamos que la vista ya no esta cacheada\n",
    "println(s\"Is person3 cached -> ${catalog.isCached(\"persons3\")}\")\n",
    "\n",
    "catalog.dropTempView(\"persons3\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2- Is df cached -> true\n",
      "2- Is df2 cached -> true\n",
      "2- Is df3 cached -> true\n",
      "3- Is df cached despues del unpersist de df -> false\n",
      "3- Is df3 cached despues del unpersist de df3 -> false\n",
      "3- Is df2 cached despues del catalog.clearCache() -> false\n"
     ]
    }
   ],
   "source": [
    "// Comprobamos que el método isCached del catalogo funciona con los métodos cache tanto de los DataFrames como de SparkSQL\n",
    "\n",
    "if (catalog.tableExists(\"df\")){ // verificamos la existencia de la tabla/vista para evitar  -> AnalysisException: Table or view not found: df;\n",
    "  println(s\"1- Is df cached -> ${catalog.isCached(\"df\")}\")\n",
    "}\n",
    "\n",
    "if (catalog.tableExists(\"df2\")){ // verificamos la existencia de la tabla/vista para evitar  -> AnalysisException: Table or view not found: df2;\n",
    "  println(s\"1- Is df2 cached -> ${catalog.isCached(\"df2\")}\")\n",
    "}\n",
    "\n",
    "if (catalog.tableExists(\"df3\")){ // verificamos la existencia de la tabla/vista para evitar  -> AnalysisException: Table or view not found: df3;\n",
    "  println(s\"1- Is df3 cached -> ${catalog.isCached(\"df3\")}\")\n",
    "}\n",
    "\n",
    "df.cache()\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "df.count()\n",
    "\n",
    "df2.cache()\n",
    "df2.createOrReplaceTempView(\"df2\")\n",
    "df2.count()\n",
    "\n",
    "df3.createOrReplaceTempView(\"df3\")\n",
    "spark.sql(\"cache table df3\")\n",
    "\n",
    "println(s\"2- Is df cached -> ${catalog.isCached(\"df\")}\")\n",
    "println(s\"2- Is df2 cached -> ${catalog.isCached(\"df2\")}\")\n",
    "println(s\"2- Is df3 cached -> ${catalog.isCached(\"df3\")}\")\n",
    "\n",
    "df.unpersist()\n",
    "df3.unpersist()\n",
    "\n",
    "println(s\"3- Is df cached despues del unpersist de df -> ${catalog.isCached(\"df\")}\")\n",
    "// Funciona la eliminación del cache tanto con df3.unpersist como con sparkSQL utilizando uncache\n",
    "println(s\"3- Is df3 cached despues del unpersist de df3 -> ${catalog.isCached(\"df3\")}\")\n",
    "\n",
    "catalog.clearCache()\n",
    "// Funciona la eliminación del cache también con catalog.clearCache()\n",
    "println(s\"3- Is df2 cached despues del catalog.clearCache() -> ${catalog.isCached(\"df2\")}\")\n",
    "\n",
    "// NOTA: Al ejecutar la misma celda una y otra vez los DataFrame.cache() no almacenan los datos en cache, \n",
    "// mientras que únicamente funcionan aquellos invocados mediante Spark SQL\n",
    "// No sé si se deba a algo propio de la plataforma o en cambio atañe a Spark\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
