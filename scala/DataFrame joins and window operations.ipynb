{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook creado para mostrar algunas de las formas de llevar a cabo joins entre DataFrames aplicando a su vez operaciones de ventana sobre los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types.{StringType, StructField, IntegerType, DateType, StructType}\n",
       "salaries_file_location: String = ../data/salaries.csv\n",
       "employees_file_location: String = ../data/employees.csv\n",
       "departments_file_location: String = ../data/departments.csv\n",
       "dept_emp_file_location: String = ../data/dept_emp.csv\n",
       "import org.apache.spark.sql.Row\n",
       "salariesSchema: org.apache.spark.sql.types.StructType = StructType(StructField(emp_no,IntegerType,true), StructField(salary,IntegerType,true), StructField(from_date,DateType,true), StructField(to_date,DateType,true))\n",
       "employeesSchema: org.apache.spark.sql.types.StructType = StructType(StructField(emp_no,IntegerType,true), StructField(birth_date,DateType,true), StructField(first_name,StringType,true), StructField(last_name,StringType,true), Str...\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types.{StringType, StructField, IntegerType, DateType, StructType}\n",
    "\n",
    "val salaries_file_location = \"../data/salaries.csv\"\n",
    "val employees_file_location = \"../data/employees.csv\"\n",
    "val departments_file_location = \"../data/departments.csv\"\n",
    "val dept_emp_file_location = \"../data/dept_emp.csv\"\n",
    "\n",
    "\n",
    "import org.apache.spark.sql.Row\n",
    "val salariesSchema = StructType(\n",
    "  Array(\n",
    "    StructField(\"emp_no\", IntegerType, true),\n",
    "    StructField(\"salary\", IntegerType, true),\n",
    "    StructField(\"from_date\", DateType, true),\n",
    "    StructField(\"to_date\", DateType, true)\n",
    "  )\n",
    ")\n",
    "\n",
    "val employeesSchema = StructType(\n",
    "  Array(\n",
    "    StructField(\"emp_no\", IntegerType, true),\n",
    "    StructField(\"birth_date\", DateType, true),\n",
    "    StructField(\"first_name\", StringType, true),\n",
    "    StructField(\"last_name\", StringType, true),\n",
    "    StructField(\"gender\", StringType, true),\n",
    "    StructField(\"hire_date\", DateType, true)\n",
    "  )\n",
    ")\n",
    "\n",
    "val departmentsSchema = StructType(\n",
    "  Array(\n",
    "    StructField(\"dept_no\", StringType, true),\n",
    "    StructField(\"dept_name\", StringType, true)\n",
    "  )\n",
    ")\n",
    "\n",
    "val deptEmpSchema = StructType(\n",
    "  Array(\n",
    "    StructField(\"emp_no\", IntegerType, true),\n",
    "    StructField(\"dept_no\", StringType, true),\n",
    "    StructField(\"from_date\", DateType, true),\n",
    "    StructField(\"to_date\", DateType, true)\n",
    "  )\n",
    ")\n",
    "  \n",
    "\n",
    "val salariesDF = spark.read.format(\"csv\")\n",
    ".option(\"inferSchema\", false)\n",
    ".option(\"header\", true)\n",
    ".option(\"ignoreLeadingWhiteSpace\", true)\n",
    ".option(\"ignoreTrailingWhiteSpace\", true)\n",
    ".option(\"sep\", \",\")\n",
    ".option(\"quote\", \"\\\"\")\n",
    ".option(\"escape\", \"\\\"\")\n",
    ".schema(salariesSchema)\n",
    ".load(salaries_file_location).cache()\n",
    "\n",
    "val employeesDF = spark.read.format(\"csv\")\n",
    ".option(\"inferSchema\", false)\n",
    ".option(\"header\", true)\n",
    ".option(\"ignoreLeadingWhiteSpace\", true)\n",
    ".option(\"ignoreTrailingWhiteSpace\", true)\n",
    ".option(\"sep\", \",\")\n",
    ".option(\"quote\", \"\\\"\")\n",
    ".option(\"escape\", \"\\\"\")\n",
    ".schema(employeesSchema)\n",
    ".load(employees_file_location).cache()\n",
    "\n",
    "val departmentsDF = spark.read.format(\"csv\")\n",
    ".option(\"inferSchema\", false)\n",
    ".option(\"header\", true)\n",
    ".option(\"ignoreLeadingWhiteSpace\", true)\n",
    ".option(\"ignoreTrailingWhiteSpace\", true)\n",
    ".option(\"sep\", \",\")\n",
    ".option(\"quote\", \"\\\"\")\n",
    ".option(\"escape\", \"\\\"\")\n",
    ".schema(departmentsSchema)\n",
    ".load(departments_file_location).cache()\n",
    "\n",
    "val deptEmpDF = spark.read.format(\"csv\")\n",
    ".option(\"inferSchema\", false)\n",
    ".option(\"header\", true)\n",
    ".option(\"ignoreLeadingWhiteSpace\", true)\n",
    ".option(\"ignoreTrailingWhiteSpace\", true)\n",
    ".option(\"sep\", \",\")\n",
    ".option(\"quote\", \"\\\"\")\n",
    ".option(\"escape\", \"\\\"\")\n",
    ".schema(deptEmpSchema)\n",
    ".load(dept_emp_file_location).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----------+----------+\n",
      "|emp_no|salary|from_date |to_date   |\n",
      "+------+------+----------+----------+\n",
      "|10001 |60117 |1986-06-26|1987-06-26|\n",
      "|10001 |62102 |1987-06-26|1988-06-25|\n",
      "|10001 |66074 |1988-06-25|1989-06-25|\n",
      "|10001 |66596 |1989-06-25|1990-06-25|\n",
      "|10001 |66961 |1990-06-25|1991-06-25|\n",
      "|10001 |71046 |1991-06-25|1992-06-24|\n",
      "|10001 |74333 |1992-06-24|1993-06-24|\n",
      "|10001 |75286 |1993-06-24|1994-06-24|\n",
      "|10001 |75994 |1994-06-24|1995-06-24|\n",
      "|10001 |76884 |1995-06-24|1996-06-23|\n",
      "|10001 |80013 |1996-06-23|1997-06-23|\n",
      "|10001 |81025 |1997-06-23|1998-06-23|\n",
      "|10001 |81097 |1998-06-23|1999-06-23|\n",
      "|10001 |84917 |1999-06-23|2000-06-22|\n",
      "|10001 |85112 |2000-06-22|2001-06-22|\n",
      "|10001 |85097 |2001-06-22|2002-06-22|\n",
      "|10001 |88958 |2002-06-22|9999-01-01|\n",
      "|10002 |65828 |1996-08-03|1997-08-03|\n",
      "|10002 |65909 |1997-08-03|1998-08-03|\n",
      "|10002 |67534 |1998-08-03|1999-08-03|\n",
      "+------+------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salariesDF.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+-----------+------+----------+\n",
      "|emp_no|birth_date|first_name|last_name  |gender|hire_date |\n",
      "+------+----------+----------+-----------+------+----------+\n",
      "|10001 |1953-09-02|Georgi    |Facello    |M     |1986-06-26|\n",
      "|10002 |1964-06-02|Bezalel   |Simmel     |F     |1985-11-21|\n",
      "|10003 |1959-12-03|Parto     |Bamford    |M     |1986-08-28|\n",
      "|10004 |1954-05-01|Chirstian |Koblick    |M     |1986-12-01|\n",
      "|10005 |1955-01-21|Kyoichi   |Maliniak   |M     |1989-09-12|\n",
      "|10006 |1953-04-20|Anneke    |Preusig    |F     |1989-06-02|\n",
      "|10007 |1957-05-23|Tzvetan   |Zielinski  |F     |1989-02-10|\n",
      "|10008 |1958-02-19|Saniya    |Kalloufi   |M     |1994-09-15|\n",
      "|10009 |1952-04-19|Sumant    |Peac       |F     |1985-02-18|\n",
      "|10010 |1963-06-01|Duangkaew |Piveteau   |F     |1989-08-24|\n",
      "|10011 |1953-11-07|Mary      |Sluis      |F     |1990-01-22|\n",
      "|10012 |1960-10-04|Patricio  |Bridgland  |M     |1992-12-18|\n",
      "|10013 |1963-06-07|Eberhardt |Terkki     |M     |1985-10-20|\n",
      "|10014 |1956-02-12|Berni     |Genin      |M     |1987-03-11|\n",
      "|10015 |1959-08-19|Guoxiang  |Nooteboom  |M     |1987-07-02|\n",
      "|10016 |1961-05-02|Kazuhito  |Cappelletti|M     |1995-01-27|\n",
      "|10017 |1958-07-06|Cristinel |Bouloucos  |F     |1993-08-03|\n",
      "|10018 |1954-06-19|Kazuhide  |Peha       |F     |1987-04-03|\n",
      "|10019 |1953-01-23|Lillian   |Haddadi    |M     |1999-04-30|\n",
      "|10020 |1952-12-24|Mayuko    |Warwick    |M     |1991-01-26|\n",
      "+------+----------+----------+-----------+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|dept_no|dept_name         |\n",
      "+-------+------------------+\n",
      "|d009   |Customer Service  |\n",
      "|d005   |Development       |\n",
      "|d002   |Finance           |\n",
      "|d003   |Human Resources   |\n",
      "|d001   |Marketing         |\n",
      "|d004   |Production        |\n",
      "|d006   |Quality Management|\n",
      "|d008   |Research          |\n",
      "|d007   |Sales             |\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "departmentsDF.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----------+----------+\n",
      "|emp_no|dept_no|from_date |to_date   |\n",
      "+------+-------+----------+----------+\n",
      "|10001 |d005   |1986-06-26|9999-01-01|\n",
      "|10002 |d007   |1996-08-03|9999-01-01|\n",
      "|10003 |d004   |1995-12-03|9999-01-01|\n",
      "|10004 |d004   |1986-12-01|9999-01-01|\n",
      "|10005 |d003   |1989-09-12|9999-01-01|\n",
      "|10006 |d005   |1990-08-05|9999-01-01|\n",
      "|10007 |d008   |1989-02-10|9999-01-01|\n",
      "|10008 |d005   |1998-03-11|2000-07-31|\n",
      "|10009 |d006   |1985-02-18|9999-01-01|\n",
      "|10010 |d004   |1996-11-24|2000-06-26|\n",
      "|10010 |d006   |2000-06-26|9999-01-01|\n",
      "|10011 |d009   |1990-01-22|1996-11-09|\n",
      "|10012 |d005   |1992-12-18|9999-01-01|\n",
      "|10013 |d003   |1985-10-20|9999-01-01|\n",
      "|10014 |d005   |1993-12-29|9999-01-01|\n",
      "|10015 |d008   |1992-09-19|1993-08-22|\n",
      "|10016 |d007   |1998-02-11|9999-01-01|\n",
      "|10017 |d001   |1993-08-03|9999-01-01|\n",
      "|10018 |d004   |1992-07-29|9999-01-01|\n",
      "|10018 |d005   |1987-04-03|1992-07-29|\n",
      "+------+-------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deptEmpDF.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----------+----------+----------+----+----------+\n",
      "|emp_no|dept_no|from_date |to_date   |row_number|rank|dense_rank|\n",
      "+------+-------+----------+----------+----------+----+----------+\n",
      "|10206 |d005   |1988-04-19|9999-01-01|1         |1   |1         |\n",
      "|10362 |d003   |1990-11-02|1997-07-16|1         |1   |1         |\n",
      "|10623 |d008   |1996-02-11|9999-01-01|1         |1   |1         |\n",
      "|10623 |d005   |1992-01-15|1996-02-11|2         |2   |2         |\n",
      "|10817 |d009   |2000-01-24|9999-01-01|1         |1   |1         |\n",
      "|10817 |d007   |1990-12-26|2000-01-24|2         |2   |2         |\n",
      "|11033 |d005   |1991-03-14|9999-01-01|1         |1   |1         |\n",
      "|11141 |d005   |1993-02-12|9999-01-01|1         |1   |1         |\n",
      "|11317 |d005   |1998-07-04|9999-01-01|1         |1   |1         |\n",
      "|11317 |d008   |1995-09-08|1998-07-04|2         |2   |2         |\n",
      "|11458 |d008   |1985-10-06|9999-01-01|1         |1   |1         |\n",
      "|11748 |d005   |1996-07-24|9999-01-01|1         |1   |1         |\n",
      "|11858 |d002   |1996-08-15|9999-01-01|1         |1   |1         |\n",
      "|12027 |d008   |1992-01-05|9999-01-01|1         |1   |1         |\n",
      "|12027 |d005   |1985-11-23|1992-01-05|2         |2   |2         |\n",
      "|12046 |d001   |1996-03-12|1998-09-26|1         |1   |1         |\n",
      "|12046 |d007   |1996-01-13|1996-03-12|2         |2   |2         |\n",
      "|12799 |d003   |1993-12-25|9999-01-01|1         |1   |1         |\n",
      "|12940 |d005   |1987-12-12|9999-01-01|1         |1   |1         |\n",
      "|13285 |d005   |1997-06-25|2000-11-02|1         |1   |1         |\n",
      "+------+-------+----------+----------+----------+----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.expressions.Window\n",
       "import org.apache.spark.sql.functions.{row_number, rank, dense_rank, col}\n",
       "windowSpec: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@7a7da919\n",
       "deptEmpSortedWithRowNumberDF: org.apache.spark.sql.DataFrame = [emp_no: int, dept_no: string ... 5 more fields]\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.expressions.Window\n",
    "import org.apache.spark.sql.functions.{row_number, rank, dense_rank, col}\n",
    "\n",
    "// Operación de ventana: se ordenada de forma descendente por fecha todos los registros por empleado\n",
    "// Se aprovecha de exploar las distitntas operaciones de ventana\n",
    "val windowSpec = Window.partitionBy(\"emp_no\").orderBy(col(\"to_date\").desc)\n",
    "val deptEmpSortedWithRowNumberDF = deptEmpDF.withColumn(\"row_number\", row_number().over(windowSpec))\n",
    "                                            .withColumn(\"rank\", rank().over(windowSpec))\n",
    "                                            .withColumn(\"dense_rank\", dense_rank().over(windowSpec))\n",
    "\n",
    "deptEmpSortedWithRowNumberDF.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----------+----------+\n",
      "|emp_no|dept_no|from_date |to_date   |\n",
      "+------+-------+----------+----------+\n",
      "|10206 |d005   |1988-04-19|9999-01-01|\n",
      "|10362 |d003   |1990-11-02|1997-07-16|\n",
      "|10623 |d008   |1996-02-11|9999-01-01|\n",
      "|10817 |d009   |2000-01-24|9999-01-01|\n",
      "|11033 |d005   |1991-03-14|9999-01-01|\n",
      "|11141 |d005   |1993-02-12|9999-01-01|\n",
      "|11317 |d005   |1998-07-04|9999-01-01|\n",
      "|11458 |d008   |1985-10-06|9999-01-01|\n",
      "|11748 |d005   |1996-07-24|9999-01-01|\n",
      "|11858 |d002   |1996-08-15|9999-01-01|\n",
      "|12027 |d008   |1992-01-05|9999-01-01|\n",
      "|12046 |d001   |1996-03-12|1998-09-26|\n",
      "|12799 |d003   |1993-12-25|9999-01-01|\n",
      "|12940 |d005   |1987-12-12|9999-01-01|\n",
      "|13285 |d005   |1997-06-25|2000-11-02|\n",
      "|13289 |d001   |1987-09-24|9999-01-01|\n",
      "|13623 |d006   |1995-10-29|9999-01-01|\n",
      "|13832 |d004   |1999-01-19|1999-05-05|\n",
      "|13840 |d005   |1998-12-12|1999-12-30|\n",
      "|14450 |d005   |1985-04-26|1996-12-30|\n",
      "+------+-------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lastDeptEmpSortedDF: org.apache.spark.sql.DataFrame = [emp_no: int, dept_no: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Nos quedamos con el registro mas reciente por empleado\n",
    "// Aprovecha de eliminar las columnas adicionales\n",
    "val lastDeptEmpSortedDF = deptEmpSortedWithRowNumberDF.where($\"row_number\" === 1)\n",
    "                                                      .drop(\"row_number\")\n",
    "                                                      .drop(\"rank\")\n",
    "                                                      .drop(\"dense_rank\")\n",
    "lastDeptEmpSortedDF.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----------+----------+\n",
      "|emp_no|salary|from_date |to_date   |\n",
      "+------+------+----------+----------+\n",
      "|10206 |71052 |2002-04-16|9999-01-01|\n",
      "|10362 |54987 |1996-10-31|1997-07-16|\n",
      "|10623 |86399 |2002-01-12|9999-01-01|\n",
      "|10817 |78202 |2001-12-23|9999-01-01|\n",
      "|11033 |75271 |2002-03-11|9999-01-01|\n",
      "|11141 |57809 |2002-02-10|9999-01-01|\n",
      "|11317 |55544 |2001-09-06|9999-01-01|\n",
      "|11458 |82731 |2001-10-02|9999-01-01|\n",
      "|11748 |70230 |2002-07-23|9999-01-01|\n",
      "|11858 |47998 |2001-08-14|9999-01-01|\n",
      "|12027 |71281 |2001-11-19|9999-01-01|\n",
      "|12046 |76043 |1998-01-12|1998-09-26|\n",
      "|12799 |59831 |2001-12-23|9999-01-01|\n",
      "|12940 |85425 |2001-12-08|9999-01-01|\n",
      "|13285 |48494 |2000-06-24|2000-11-02|\n",
      "|13289 |85488 |2001-09-20|9999-01-01|\n",
      "|13623 |51145 |2001-10-27|9999-01-01|\n",
      "|13832 |77530 |1998-10-05|1999-05-05|\n",
      "|13840 |41453 |1999-12-12|1999-12-30|\n",
      "|14450 |75419 |1996-04-23|1996-12-30|\n",
      "+------+------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lastSalariesDF: org.apache.spark.sql.DataFrame = [emp_no: int, salary: int ... 2 more fields]\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Utilizamos la misma operación de ventana para quedarnos con el salario más reciente de cada empelado\n",
    "val lastSalariesDF = salariesDF.withColumn(\"row_number\", row_number().over(windowSpec))\n",
    "                              .where($\"row_number\" === 1)\n",
    "                              .drop(\"row_number\")\n",
    "\n",
    "lastSalariesDF.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+------+------+----------+----------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|emp_no|salary| from_date|   to_date|\n",
      "+------+----------+----------+---------+------+----------+------+------+----------+----------+\n",
      "| 10206|1960-09-19|  Alassane|  Iwayama|     F|1988-04-19| 10206| 71052|2002-04-16|9999-01-01|\n",
      "| 10362|1963-09-16|   Shalesh|  dAstous|     M|1988-08-24| 10362| 54987|1996-10-31|1997-07-16|\n",
      "| 10623|1953-07-11|Aleksander|   Danlos|     F|1987-03-07| 10623| 86399|2002-01-12|9999-01-01|\n",
      "| 10817|1958-10-02|       Uri|  Rullman|     F|1990-12-26| 10817| 78202|2001-12-23|9999-01-01|\n",
      "| 11033|1957-03-01|   Shushma|     Bahk|     F|1990-10-02| 11033| 75271|2002-03-11|9999-01-01|\n",
      "| 11141|1957-08-20|   Vasiliy|Kermarrec|     F|1989-12-28| 11141| 57809|2002-02-10|9999-01-01|\n",
      "| 11317|1954-07-24|  Shigeaki| Hagimont|     F|1989-12-21| 11317| 55544|2001-09-06|9999-01-01|\n",
      "| 11458|1958-08-09|     Stevo|Chenoweth|     F|1985-10-06| 11458| 82731|2001-10-02|9999-01-01|\n",
      "| 11748|1953-03-07|    Lihong| Massonet|     M|1992-12-20| 11748| 70230|2002-07-23|9999-01-01|\n",
      "| 11858|1962-11-21|   Slavian|     Baik|     M|1988-11-12| 11858| 47998|2001-08-14|9999-01-01|\n",
      "| 12027|1962-07-31|   Zhanqiu| Vuskovic|     M|1985-11-23| 12027| 71281|2001-11-19|9999-01-01|\n",
      "| 12046|1961-05-16|     Xiong|    Ranum|     F|1996-01-13| 12046| 76043|1998-01-12|1998-09-26|\n",
      "| 12799|1954-11-21|     Tiina| Businaro|     F|1993-12-25| 12799| 59831|2001-12-23|9999-01-01|\n",
      "| 12940|1953-10-25|  Odinaldo|   Farrar|     F|1987-12-12| 12940| 85425|2001-12-08|9999-01-01|\n",
      "| 13285|1963-03-06|     Uinam|Lienhardt|     M|1990-03-13| 13285| 48494|2000-06-24|2000-11-02|\n",
      "| 13289|1962-08-12|     Berni|     Baer|     F|1987-09-24| 13289| 85488|2001-09-20|9999-01-01|\n",
      "| 13623|1961-05-24|  Breannda|    Yeung|     M|1995-03-02| 13623| 51145|2001-10-27|9999-01-01|\n",
      "| 13832|1954-04-21|  Hidefumi| Siepmann|     F|1990-02-15| 13832| 77530|1998-10-05|1999-05-05|\n",
      "| 13840|1954-11-13|     Remco|    Demke|     M|1992-06-09| 13840| 41453|1999-12-12|1999-12-30|\n",
      "| 14450|1963-08-01|  Fumitaka|Prochazka|     F|1985-04-26| 14450| 75419|1996-04-23|1996-12-30|\n",
      "+------+----------+----------+---------+------+----------+------+------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "employeesWithSalaryTempDF: org.apache.spark.sql.DataFrame = [emp_no: int, birth_date: date ... 8 more fields]\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Hacemos join entre employeesDF y lastSalariesDF. De está forma enriquecemos el DF de empleados añadiendole a c/u su salario más reciente\n",
    "// Esta forma de hacer el join se indica la expresión o condición de emparejamiento entre ambos DF\n",
    "// EL hacerlo de esta forma hace que aparezcan todas las columnas de cada DF sin importar que se llamen igual y sean las utilizadas en la expresión del join\n",
    "// Cuando se omite el tipo de join se asume que es un inner join\n",
    "val employeesWithSalaryTempDF = employeesDF.join(lastSalariesDF, employeesDF(\"emp_no\") === lastSalariesDF(\"emp_no\"))\n",
    "employeesWithSalaryTempDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+------+\n",
      "|emp_no|birth_date|first_name|last_name|gender|hire_date |salary|\n",
      "+------+----------+----------+---------+------+----------+------+\n",
      "|10206 |1960-09-19|Alassane  |Iwayama  |F     |1988-04-19|71052 |\n",
      "|10362 |1963-09-16|Shalesh   |dAstous  |M     |1988-08-24|54987 |\n",
      "|10623 |1953-07-11|Aleksander|Danlos   |F     |1987-03-07|86399 |\n",
      "|10817 |1958-10-02|Uri       |Rullman  |F     |1990-12-26|78202 |\n",
      "|11033 |1957-03-01|Shushma   |Bahk     |F     |1990-10-02|75271 |\n",
      "|11141 |1957-08-20|Vasiliy   |Kermarrec|F     |1989-12-28|57809 |\n",
      "|11317 |1954-07-24|Shigeaki  |Hagimont |F     |1989-12-21|55544 |\n",
      "|11458 |1958-08-09|Stevo     |Chenoweth|F     |1985-10-06|82731 |\n",
      "|11748 |1953-03-07|Lihong    |Massonet |M     |1992-12-20|70230 |\n",
      "|11858 |1962-11-21|Slavian   |Baik     |M     |1988-11-12|47998 |\n",
      "|12027 |1962-07-31|Zhanqiu   |Vuskovic |M     |1985-11-23|71281 |\n",
      "|12046 |1961-05-16|Xiong     |Ranum    |F     |1996-01-13|76043 |\n",
      "|12799 |1954-11-21|Tiina     |Businaro |F     |1993-12-25|59831 |\n",
      "|12940 |1953-10-25|Odinaldo  |Farrar   |F     |1987-12-12|85425 |\n",
      "|13285 |1963-03-06|Uinam     |Lienhardt|M     |1990-03-13|48494 |\n",
      "|13289 |1962-08-12|Berni     |Baer     |F     |1987-09-24|85488 |\n",
      "|13623 |1961-05-24|Breannda  |Yeung    |M     |1995-03-02|51145 |\n",
      "|13832 |1954-04-21|Hidefumi  |Siepmann |F     |1990-02-15|77530 |\n",
      "|13840 |1954-11-13|Remco     |Demke    |M     |1992-06-09|41453 |\n",
      "|14450 |1963-08-01|Fumitaka  |Prochazka|F     |1985-04-26|75419 |\n",
      "+------+----------+----------+---------+------+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "employeesWithSalaryDF: org.apache.spark.sql.DataFrame = [emp_no: int, birth_date: date ... 5 more fields]\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Para evitar errores y duplicidad de datos nos quedamos con algunas columnas y escojemos una sola de cnúmero de empleado\n",
    "// Por ello es necesario indicar a que DF pertenece la columna emp_no para evitar resolviendo el conflicto\n",
    "val employeesWithSalaryDF = employeesWithSalaryTempDF.select(employeesDF(\"emp_no\"), col(\"birth_date\"), col(\"first_name\"), col(\"last_name\"), col(\"gender\"), col(\"hire_date\"), col(\"salary\"))\n",
    "employeesWithSalaryDF.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------+----------+----------+\n",
      "|dept_no|dept_name         |emp_no|from_date |to_date   |\n",
      "+-------+------------------+------+----------+----------+\n",
      "|d005   |Development       |10206 |1988-04-19|9999-01-01|\n",
      "|d003   |Human Resources   |10362 |1990-11-02|1997-07-16|\n",
      "|d008   |Research          |10623 |1996-02-11|9999-01-01|\n",
      "|d009   |Customer Service  |10817 |2000-01-24|9999-01-01|\n",
      "|d005   |Development       |11033 |1991-03-14|9999-01-01|\n",
      "|d005   |Development       |11141 |1993-02-12|9999-01-01|\n",
      "|d005   |Development       |11317 |1998-07-04|9999-01-01|\n",
      "|d008   |Research          |11458 |1985-10-06|9999-01-01|\n",
      "|d005   |Development       |11748 |1996-07-24|9999-01-01|\n",
      "|d002   |Finance           |11858 |1996-08-15|9999-01-01|\n",
      "|d008   |Research          |12027 |1992-01-05|9999-01-01|\n",
      "|d001   |Marketing         |12046 |1996-03-12|1998-09-26|\n",
      "|d003   |Human Resources   |12799 |1993-12-25|9999-01-01|\n",
      "|d005   |Development       |12940 |1987-12-12|9999-01-01|\n",
      "|d005   |Development       |13285 |1997-06-25|2000-11-02|\n",
      "|d001   |Marketing         |13289 |1987-09-24|9999-01-01|\n",
      "|d006   |Quality Management|13623 |1995-10-29|9999-01-01|\n",
      "|d004   |Production        |13832 |1999-01-19|1999-05-05|\n",
      "|d005   |Development       |13840 |1998-12-12|1999-12-30|\n",
      "|d005   |Development       |14450 |1985-04-26|1996-12-30|\n",
      "+-------+------------------+------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lastDeptWithNameEmpDF: org.apache.spark.sql.DataFrame = [dept_no: string, dept_name: string ... 3 more fields]\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Otra forma de llevar a cabo un Join para enriquecer el DF con el departamente del empleado \n",
    "// En este caso se indican el conj de columnas utilizadas como expresión del Join\n",
    "// De esta forma se evita duplicidad de columnas especialmente en aquellas que forman parte del Join, por ejemplo emp_no\n",
    "// Una forma de expresar el tipo de Join es mediante un String por ejemplo \"inner\"\n",
    "val lastDeptWithNameEmpDF = departmentsDF.join(lastDeptEmpSortedDF, Seq(\"dept_no\"), \"inner\")\n",
    "            .select(departmentsDF.col(\"dept_no\"), col(\"dept_name\"), col(\"emp_no\"), col(\"from_date\"), col(\"to_date\"))\n",
    "lastDeptWithNameEmpDF.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+------+-------+------------------+----------+----------+\n",
      "|emp_no|birth_date|first_name|last_name|gender|hire_date |salary|dept_no|dept_name         |from_date |to_date   |\n",
      "+------+----------+----------+---------+------+----------+------+-------+------------------+----------+----------+\n",
      "|10206 |1960-09-19|Alassane  |Iwayama  |F     |1988-04-19|71052 |d005   |Development       |1988-04-19|9999-01-01|\n",
      "|10362 |1963-09-16|Shalesh   |dAstous  |M     |1988-08-24|54987 |d003   |Human Resources   |1990-11-02|1997-07-16|\n",
      "|10623 |1953-07-11|Aleksander|Danlos   |F     |1987-03-07|86399 |d008   |Research          |1996-02-11|9999-01-01|\n",
      "|10817 |1958-10-02|Uri       |Rullman  |F     |1990-12-26|78202 |d009   |Customer Service  |2000-01-24|9999-01-01|\n",
      "|11033 |1957-03-01|Shushma   |Bahk     |F     |1990-10-02|75271 |d005   |Development       |1991-03-14|9999-01-01|\n",
      "|11141 |1957-08-20|Vasiliy   |Kermarrec|F     |1989-12-28|57809 |d005   |Development       |1993-02-12|9999-01-01|\n",
      "|11317 |1954-07-24|Shigeaki  |Hagimont |F     |1989-12-21|55544 |d005   |Development       |1998-07-04|9999-01-01|\n",
      "|11458 |1958-08-09|Stevo     |Chenoweth|F     |1985-10-06|82731 |d008   |Research          |1985-10-06|9999-01-01|\n",
      "|11748 |1953-03-07|Lihong    |Massonet |M     |1992-12-20|70230 |d005   |Development       |1996-07-24|9999-01-01|\n",
      "|11858 |1962-11-21|Slavian   |Baik     |M     |1988-11-12|47998 |d002   |Finance           |1996-08-15|9999-01-01|\n",
      "|12027 |1962-07-31|Zhanqiu   |Vuskovic |M     |1985-11-23|71281 |d008   |Research          |1992-01-05|9999-01-01|\n",
      "|12046 |1961-05-16|Xiong     |Ranum    |F     |1996-01-13|76043 |d001   |Marketing         |1996-03-12|1998-09-26|\n",
      "|12799 |1954-11-21|Tiina     |Businaro |F     |1993-12-25|59831 |d003   |Human Resources   |1993-12-25|9999-01-01|\n",
      "|12940 |1953-10-25|Odinaldo  |Farrar   |F     |1987-12-12|85425 |d005   |Development       |1987-12-12|9999-01-01|\n",
      "|13285 |1963-03-06|Uinam     |Lienhardt|M     |1990-03-13|48494 |d005   |Development       |1997-06-25|2000-11-02|\n",
      "|13289 |1962-08-12|Berni     |Baer     |F     |1987-09-24|85488 |d001   |Marketing         |1987-09-24|9999-01-01|\n",
      "|13623 |1961-05-24|Breannda  |Yeung    |M     |1995-03-02|51145 |d006   |Quality Management|1995-10-29|9999-01-01|\n",
      "|13832 |1954-04-21|Hidefumi  |Siepmann |F     |1990-02-15|77530 |d004   |Production        |1999-01-19|1999-05-05|\n",
      "|13840 |1954-11-13|Remco     |Demke    |M     |1992-06-09|41453 |d005   |Development       |1998-12-12|1999-12-30|\n",
      "|14450 |1963-08-01|Fumitaka  |Prochazka|F     |1985-04-26|75419 |d005   |Development       |1985-04-26|1996-12-30|\n",
      "+------+----------+----------+---------+------+----------+------+-------+------------------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.catalyst.plans._\n",
       "finalEmployeesEnrichedDF: org.apache.spark.sql.DataFrame = [emp_no: int, birth_date: date ... 9 more fields]\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.catalyst.plans._\n",
    "\n",
    "// Otra forma de expresar el Join es mediante unas constantes \n",
    "// de esta forma se puede evitar errores en tiempo de ejecución al indicar mal un tipo de Join\n",
    "val finalEmployeesEnrichedDF = employeesWithSalaryDF.join(lastDeptWithNameEmpDF, Seq(\"emp_no\"), Inner.sql)\n",
    "finalEmployeesEnrichedDF.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300024\n",
      "300024\n"
     ]
    }
   ],
   "source": [
    "println(employeesDF.count)\n",
    "println(finalEmployeesEnrichedDF.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
